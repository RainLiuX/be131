{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7\n",
    "\n",
    "## Background\n",
    "\n",
    "In a biotech company that generates 1000 terabytes of data every day, the storage costs huge amount of money. My goal is to find a balance between time for compression and the compression rete, to determine the algorithm that compresses the maximum amount of data every day.\n",
    "\n",
    "## Simulating the data\n",
    "\n",
    "In this section, I will simulate 3 types of data: pure 0 or 1, nucleotide sequence, and protein sequence, in order to test how much each of them can be compressed by different algorithms.\n",
    "\n",
    "First, generate **100 megabytes** of 0/1 data. A length 1 0/1 data takes up **1 bit** to store. A 100 megabytes file takes up $8\\times1024^2\\times100$ bits, and that is also the length of this 0/1 string. Note that we store the data as binary file instead of a text file, each 0/1 only takes up **1 byte** rather than 1 byte (an ascii character).\n",
    "\n",
    "As is required, I have generated data of different 0 ratio from 100% to 50%. Different frequency of 0 reslults in different information entropy. The data with larger 0 frenquency (larger than 50%) has **less entropy**, and are expected to have **larger compression rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate 1MB random 01\n",
    "ratio_list = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "STR_LENGTH = 8 * 1024 * 1024 * 100\n",
    "for ratio in ratio_list :\n",
    "    myvar = np.random.choice([0, 1], size=STR_LENGTH, replace=True, p=[ratio, 1.0-ratio])\n",
    "    #replace parameter controls whether there is repeated element in the list\n",
    "    myvar = np.packbits(myvar)\n",
    "    _ = open('zeros_100M_'+str(int(ratio*100)), 'wb').write(myvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate a random DNA sequence that is 100,000,000 letters long. The actual file size is $10^8 bytes = 10^8/1024/1024 MB \\approx 95.4 MB$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate 100 million long DNA sequence\n",
    "mydna = np.random.choice(['A', 'T', 'C', 'G'], size=100000000,\\\n",
    "                        replace=True, p=[0.25, 0.25, 0.25, 0.25])\n",
    "_ = open('nt_seq', 'w').write(\"\".join(mydna))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, generate a random sequence of protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate 100 million long protein sequence\n",
    "mypro = np.random.choice(['A','R','N','D','C','E','Q','G','H','I','L','K','M','F','P','S','T','W','Y','V'], size=100000000,\\\n",
    "                        replace=True, p=[0.05] * 20)\n",
    "_ = open('protein_seq', 'w').write(\"\".join(mypro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing the data\n",
    "\n",
    "We are using gzip, bzip, pbzip2 and ArithmeticCompress algorithms respectively to compress the 0/1 sequence, DNA sequence and protein sequence. We are also using the `time` command in shell linux to measure time spend in compression. As we have several data for this compress test, and it is obvious that the procedure of compress text for each is the same, we can save the commands in a shell script.\n",
    "\n",
    "Here is a copy of `compress.sh`:\n",
    "```sh\n",
    "#!/bin/bash\n",
    "\n",
    "cmp_file=$1\n",
    "out_file=comp_${cmp_file}.output\n",
    "rm -f ${cmp_file}.* ${out_file}\n",
    "echo \"Time for gzip\" >> ${out_file}\n",
    "{ time -p gzip -k ${cmp_file}; } 2>> ${out_file}\n",
    "echo \"Time for bzip2\" >> ${out_file}\n",
    "{ time -p bzip2 -k ${cmp_file}; } 2>> ${out_file}\n",
    "echo \"Time for pbzip2\" >> ${out_file}\n",
    "{ time -p pbzip2 -ck ${cmp_file} > ${cmp_file}.pbz2; } 2>> ${out_file}\n",
    "echo \"Time for Arithmetic compress\" >> ${out_file}\n",
    "{ time -p ArithmeticCompress ${cmp_file} ${cmp_file}.art; } 2>> ${out_file}\n",
    "echo \"Sizes of each file:\" >> ${out_file}\n",
    "ls -l {cmp_file}* >> ${out_file}\n",
    "```\n",
    "\n",
    "There is something worth noticing. Firstly, the `time` command outputs 3 different times, respectively the **real**, **user** and **system** time. The **real** times represents the clock time elapsed to run this command, it is not very fair for commands because it includes I/O time, and some commands takes multiples cores to run but others are not. The best way to count the **real CPU time** of a command is use the **sum of user and system** time.\n",
    "\n",
    "Second, if you are redirecting the output of compression command like `gzip`, remember to use `stderr` instead of `stdout`, because `stdout` are reserved for output of compressed file.\n",
    "\n",
    "Use this script to perform compression test upon every dataset (this might take some time). For example, to test random nucleotide file we will run:\n",
    "```sh\n",
    "./compress.sh nt_seq\n",
    "```\n",
    "Then we use the following python code to extract important information for each compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   gzip     bzip2    pbzip2  arithmetic\n",
      "zeros_100M_100   0.7200   1.01000   1.90000      14.790\n",
      "zeros_100M_90   18.9900  10.69000  19.22000      28.720\n",
      "zeros_100M_80   13.4000  12.00000  24.39000      35.400\n",
      "zeros_100M_70    6.0100  13.84000  30.15000      39.390\n",
      "zeros_100M_60    4.2800  15.82000  37.21000      40.930\n",
      "zeros_100M_50    3.4900  16.73000  40.48000      40.720\n",
      "nt_seq          12.1500   9.47000  16.55000      21.270\n",
      "protein_seq      4.2600   9.95000  18.93000      28.580\n",
      "average          7.9125  11.18875  23.60375      31.225\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "data_list = ['zeros_100M_100', 'zeros_100M_90', 'zeros_100M_80', 'zeros_100M_70', 'zeros_100M_60', 'zeros_100M_50',\\\n",
    "            'nt_seq', 'protein_seq']\n",
    "method_list = ['gzip', 'bzip2', 'pbzip2', 'arithmetic']\n",
    "table = []\n",
    "for data in data_list :\n",
    "    in_file = open('comp_'+data+'.output', 'r').readlines()\n",
    "    row = []\n",
    "    row.append(float(in_file[2].split(' ')[1]) + float(in_file[3].split(' ')[1]))\n",
    "    row.append(float(in_file[6].split(' ')[1]) + float(in_file[7].split(' ')[1]))\n",
    "    row.append(float(in_file[10].split(' ')[1]) + float(in_file[11].split(' ')[1]))\n",
    "    row.append(float(in_file[14].split(' ')[1]) + float(in_file[15].split(' ')[1]))\n",
    "    table.append(row)\n",
    "df_time = pandas.DataFrame(table, data_list, method_list)\n",
    "df_time.loc['average'] = df_time.mean(axis=0)\n",
    "print(df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    gzip     bzip2    pbzip2  arithmetic\n",
      "zeros_100M_100  0.999999  0.999029  0.999946    0.999990\n",
      "zeros_100M_90   0.416744  0.439846  0.416518    0.531003\n",
      "zeros_100M_80   0.173681  0.225996  0.173549    0.278057\n",
      "zeros_100M_70   0.048591  0.107197  0.048534    0.118700\n",
      "zeros_100M_60  -0.000504  0.023263 -0.000572    0.029054\n",
      "zeros_100M_50  -0.004422 -0.000160 -0.004476   -0.000010\n",
      "nt_seq          0.726622  0.707774  0.726529    0.749990\n",
      "protein_seq     0.447355  0.394414  0.447399    0.459749\n",
      "average         0.351008  0.362170  0.350928    0.395817\n"
     ]
    }
   ],
   "source": [
    "table = []\n",
    "for data in data_list :\n",
    "    in_file = open('comp_'+data+'.output', 'r').readlines()\n",
    "    ori = int(in_file[17].split()[4])\n",
    "    row = []\n",
    "    row.append(1-int(in_file[19].split()[4]) / ori)\n",
    "    row.append(1-int(in_file[20].split()[4]) / ori)\n",
    "    row.append(1-int(in_file[21].split()[4]) / ori)\n",
    "    row.append(1-int(in_file[18].split()[4]) / ori)\n",
    "    table.append(row)\n",
    "df_comp = pandas.DataFrame(table, data_list, method_list)\n",
    "df_comp.loc['average'] = df_comp.mean(axis=0)\n",
    "print(df_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the time and compression rate of each data and each method, along with the average.\n",
    "\n",
    "1. In compressing 0/1 data, arithmetic compression generally has the best compression rate.  \n",
    "    For DNA and protein compression, arithmetic compression all works the best. However, it is also the slowest.\n",
    "2. Considering the average compression time of all data, gzip is the fastest.\n",
    "3. `pbzip2` is the parallel version of `bzip2`, so pbzip2 should be faster.For example, pbzip2 used only **0.65 seconds** to compress DNA data, but bzip2 used **9.47 seconds**, considering the **real time**.  \n",
    "    In the above table, we take the **total CPU time** for each algorithm, so pbzip2 is not faster than bzip2. Instead, pbzip2 is even **slower** than bzip2!\n",
    "4. As is predicted before, as the 0 rate goes up, the compression rate also goes up, that is because the information entrypy of file decreases as the rate of 0 increases, which means the minimum length required for coding also decreases.\n",
    "5. Theoretically, a single DNA base requires 2 bits to store, and a single amino acid requires $log_2(20) \\approx 4.32$bits to store.\n",
    "6. The actual file size for compressed DNA file is 27337817 bytes (bzip2) and 29222581 bytes (gzip). Which the actual coding length is $27337817\\times8/100000000 = 2.19$ (bzip2) and $29222581\\times8/100000000 = 2.34$ (gzip)  \n",
    "    File size for compressed protein file is 55264525 bytes (bzip2) and 60558615 bytes (gzip). Thus the actual coding length is $55264525\\times8/100000000=4.42$ (bzip2) and $60558615\\times8/100000000=4.84$  \n",
    "    Of course, these lengths are larger than Shannon limit, but they are quite close to that, which means that our compression algorithms are doing rather good. In DNA or protein sequences, bzip2 overall is doing better than gzip.\n",
    "    \n",
    "## Compressing real data\n",
    "\n",
    "Now, we are getting 10 different homologues of **gp120** protein nucleotide sequence of HIV into a single multiple sequence fasta file. Then we perform the same method as before the test the efficiency of each compression algorithm over this file. This file is no more \"random\" nucleotide bases, so it is expected to have some **conserved structures**. This structure reduces the entropy in this file, so it is expected to have **a larger compresson rate** than random one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            gp120 fasta\n",
      "gzip           0.733647\n",
      "bzip2          0.745334\n",
      "pbzip2         0.733647\n",
      "arithmetic     0.481740\n"
     ]
    }
   ],
   "source": [
    "in_file = open('comp_gp120.fa.output', 'r').readlines()\n",
    "ori = int(in_file[17].split()[4])\n",
    "row = []\n",
    "row.append(1-int(in_file[19].split()[4]) / ori)\n",
    "row.append(1-int(in_file[20].split()[4]) / ori)\n",
    "row.append(1-int(in_file[21].split()[4]) / ori)\n",
    "row.append(1-int(in_file[18].split()[4]) / ori)\n",
    "df_gp = pandas.DataFrame(row, method_list, ['gp120 fasta'])\n",
    "print(df_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to complete random files, the compression rate of gp120 fasta file **increases by 1-3 percent** overall. However, the compression rate for arithmetic compression drops sharply, which suggests it does not apply to small data set. Also, since this file is very small, the time comsuming is less than 10ms, which can be omitted.\n",
    "\n",
    "## Estimating compression of 1000 terabytes\n",
    "\n",
    "Since the data is very large, arithmetic compression is expected to get the best compressoin rate, but is the time feasible? We assume that the coding has an O(N) time complexity. So the total time required for 1000TB data is $10000000\\times(0.8\\times21.27+0.1\\times28.58+0.1\\times40.72)/3600=116 hours$. Far larger than hours we have in a day. So the time consuming for arithmetic compression is not tolerable.\n",
    "\n",
    "Accroding to the gp120 example, we suggest that **bzip2 compresses the best** for real sequences. Using this method, the expected compressing time is: $10000000\\times(0.8\\times9.47+0.1\\times9.95+0.1\\times16.73)/3600=28 hours$. This time is tolerable if we are using multiple cores (don't forget the time above derives from CPU time instead of real clock time). And the expected compression rate is $0.8\\times0.745+0.1\\times0.394+0.1\\times0.000=0.635$. That is **63.5% of space can be saved** in total.\n",
    "\n",
    "500 dollars bonus is expected from every 1\\% decrease of storage space. So in this year I will receive roughly $500\\times63.5\\times365=11.6\\ million\\ dollars$! Feels so good to be a millionaire just by compressing data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
